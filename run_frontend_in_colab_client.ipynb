{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# download nltk stopwords\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import sys\n",
        "from collections import Counter, OrderedDict\n",
        "import itertools\n",
        "from itertools import islice, count, groupby\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from operator import itemgetter\n",
        "from time import time\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "from google.cloud import storage\n",
        "from collections import defaultdict\n",
        "from contextlib import closing\n",
        "import math\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "id": "bCPDHP7zTQJZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ec949f3-6a29-4c6e-fe4e-99b0275fa8b2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install a particular version of `google-cloud-storage` because (oddly enough)\n",
        "# the  version on Colab and GCP is old. A dependency error below is okay.\n",
        "!pip install -q google-cloud-storage==1.43.0"
      ],
      "metadata": {
        "id": "lAt6KT8xOgHH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# authenticate below for Google Storage access as needed\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "-oKFly5jFLFn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aeoj64DuuGbx",
        "outputId": "10faff9e-704b-4df8-8e0d-a31815f8bc09"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.1.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: sign up for an ngrok account\n",
        "# then put your ngrok token below and execute\n",
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"2cXd1YwtWlYn3kGKfj97UtzuCER_42NZiY3AoMj93W18LwJWp\")"
      ],
      "metadata": {
        "id": "3E0IJAHOuDG1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the app"
      ],
      "metadata": {
        "id": "6dW0y91OVu5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# you need to upload your implementation of search_app.py\n",
        "import search_frontend as se\n",
        "import sim_calc\n",
        "import inverted_index_title_gcp\n",
        "import inverted_index_body_gcp_training"
      ],
      "metadata": {
        "id": "7opNkV6uRHIv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment the code below and execute to reload the module when you make\n",
        "# changes to search_frontend.py (after you upload again).\n",
        "import importlib\n",
        "importlib.reload(se)\n",
        "importlib.reload(sim_calc)\n",
        "importlib.reload(inverted_index_title_gcp)\n",
        "importlib.reload(inverted_index_body_gcp_training)"
      ],
      "metadata": {
        "id": "oTGXXYEXV5l8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fea922dc-c588-4a59-e1af-870e9c7ee3f6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'inverted_index_body_gcp_training' from '/content/inverted_index_body_gcp_training.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "public_url = ngrok.connect(\"5000\", bind_tls=False).public_url\n",
        "print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:5000\\\"\")\n",
        "# Update any base URLs to use the public ngrok URL\n",
        "se.app.config[\"BASE_URL\"] = public_url\n",
        "se.app.run()"
      ],
      "metadata": {
        "id": "J5n9u9rFP_wD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd10643a-6657-478e-dde8-4e4d1755c542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * ngrok tunnel \"http://d116-34-80-115-88.ngrok-free.app\" -> \"http://127.0.0.1:5000\"\n",
            " * Serving Flask app 'search_frontend'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [19/Feb/2024 00:52:56] \"\u001b[33mGET / HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [19/Feb/2024 00:52:57] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [19/Feb/2024 00:53:03] \"GET /search HTTP/1.1\" 200 -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing your app\n",
        "\n",
        "Once your app is running you can query it. You can simply do that by navigating to the URL that ngrok gave you above or through code in a different python session. For example, once the frontend app is running, you can navigate to:\n",
        "http://YOUR_SERVER_DOMAIN/search?query=hello+world where YOUR_SERVER_DOMAIN is something like XXXX-XX-XX-XX-XX.ngrok.io, which is printed above in Colab or that is your external IP on GCP.\n",
        "\n",
        "The code below shows how to issue a query from python. This is also how our testing code will issue queries to your search engine, so make sure to test your search engine this way after you deploy it to GCP and before submission. Command line instructions for deploying your search engine to GCP are available at `run_frontend_in_gcp.sh`. Note that we will not only issue training queries to your search engine, but also test queries, i.e. queries that you've never seen before."
      ],
      "metadata": {
        "id": "Na0MC_1nzDbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('queries_train.json', 'rt') as f:\n",
        "  queries = json.load(f)"
      ],
      "metadata": {
        "id": "EM5ePrRHojbG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def average_precision(true_list, predicted_list, k=40):\n",
        "    true_set = frozenset(true_list)\n",
        "    predicted_list = predicted_list[:k]\n",
        "    precisions = []\n",
        "    for i,doc_id in enumerate(predicted_list):\n",
        "        if doc_id in true_set:\n",
        "            prec = (len(precisions)+1) / (i+1)\n",
        "            precisions.append(prec)\n",
        "    if len(precisions) == 0:\n",
        "        return 0.0\n",
        "    return round(sum(precisions)/len(precisions),3)"
      ],
      "metadata": {
        "id": "gWimZWCOy3Ei"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def precision_at_k(true_list, predicted_list, k):\n",
        "    true_set = frozenset(true_list)\n",
        "    predicted_list = predicted_list[:k]\n",
        "    if len(predicted_list) == 0:\n",
        "        return 0.0\n",
        "    return round(len([1 for doc_id in predicted_list if doc_id in true_set]) / len(predicted_list), 3)\n",
        "def recall_at_k(true_list, predicted_list, k):\n",
        "    true_set = frozenset(true_list)\n",
        "    predicted_list = predicted_list[:k]\n",
        "    if len(true_set) < 1:\n",
        "        return 1.0\n",
        "    return round(len([1 for doc_id in predicted_list if doc_id in true_set]) / len(true_set), 3)\n",
        "def f1_at_k(true_list, predicted_list, k):\n",
        "    p = precision_at_k(true_list, predicted_list, k)\n",
        "    r = recall_at_k(true_list, predicted_list, k)\n",
        "    if p == 0.0 or r == 0.0:\n",
        "        return 0.0\n",
        "    return round(2.0 / (1.0/p + 1.0/r), 3)\n",
        "def results_quality(true_list, predicted_list):\n",
        "    p5 = precision_at_k(true_list, predicted_list, 5)\n",
        "    print(p5)\n",
        "    f1_30 = f1_at_k(true_list, predicted_list, 30)\n",
        "    print(f1_30)\n",
        "    if p5 == 0.0 or f1_30 == 0.0:\n",
        "        return 0.0\n",
        "    return round(2.0 / (1.0/p5 + 1.0/f1_30), 3)\n",
        "\n",
        "# assert precision_at_k(range(10), [1,2,3] , 2) == 1.0\n",
        "# assert recall_at_k(   range(10), [10,5,3], 2) == 0.1\n",
        "# assert precision_at_k(range(10), []      , 2) == 0.0\n",
        "# assert precision_at_k([],        [1,2,3],  5) == 0.0\n",
        "# assert recall_at_k(   [],        [10,5,3], 2) == 1.0\n",
        "# assert recall_at_k(   range(10), [],       2) == 0.0\n",
        "# assert f1_at_k(       [],        [1,2,3],  5) == 0.0\n",
        "# assert f1_at_k(       range(10), [],       2) == 0.0\n",
        "# assert f1_at_k(       range(10), [0,1,2],  2) == 0.333\n",
        "# assert f1_at_k(       range(50), range(5), 30) == 0.182\n",
        "# assert f1_at_k(       range(50), range(10), 30) == 0.333\n",
        "# assert f1_at_k(       range(50), range(30), 30) == 0.75\n",
        "# assert results_quality(range(50), range(5))  == 0.308\n",
        "# assert results_quality(range(50), range(10)) == 0.5\n",
        "# assert results_quality(range(50), range(30)) == 0.857\n",
        "# assert results_quality(range(50), [-1]*5 + list(range(5,30))) == 0.0\n"
      ],
      "metadata": {
        "id": "geHKyFB4xkBe"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_stopwords = frozenset(stopwords.words('english'))\n",
        "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n",
        "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
        "                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
        "                    \"many\", \"however\", \"would\", \"became\"]\n",
        "\n",
        "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
        "\n",
        "def tokenize(text):\n",
        "    RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
        "    tokens = [token.group() for token in RE_WORD.finditer(text.lower()) if token.group() not in all_stopwords]\n",
        "    print(tokens)\n",
        "    return tokens\n",
        "\n",
        "def get_synonyms(word):\n",
        "    synonyms = []\n",
        "    # if isinstance(word, tuple):\n",
        "    #     filtered_string = ' '.join([str(item) for item in word if isinstance(item, str)])\n",
        "    #     for syn in wordnet.synsets(filtered_string):\n",
        "    #         for lemma in syn.lemmas():\n",
        "    #             # Get the normalized form of the word from the lemma\n",
        "    #             normalized_word = lemma.name()\n",
        "    #             # Check if the word contains any signs\n",
        "    #             if normalized_word.isalpha():\n",
        "    #                 # Remove characters specified by the regular expression pattern\n",
        "    #                 normalized_word = ' '.join(RE_WORD.findall(normalized_word))\n",
        "    #                 normalized_word = normalized_word.lower()\n",
        "    #                 if normalized_word:\n",
        "    #                     synonyms.append(normalized_word)\n",
        "    #     return synonyms\n",
        "    # else:\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            # Get the normalized form of the word from the lemma\n",
        "            normalized_word = lemma.name()\n",
        "            # Check if the word contains any signs\n",
        "            if normalized_word.isalpha():\n",
        "                # Remove characters specified by the regular expression pattern\n",
        "                # normalized_word = ' '.join(RE_WORD.findall(normalized_word))\n",
        "                normalized_word = normalized_word.lower()\n",
        "                if normalized_word:\n",
        "                    synonyms.append(normalized_word)\n",
        "    return synonyms\n",
        "\n",
        "def find_most_similar(word, n=5):\n",
        "    synonyms = get_synonyms(word)\n",
        "    similarities = [(syn, wordnet.wup_similarity(wordnet.synsets(word)[0], wordnet.synsets(syn)[0]))\n",
        "                    for syn in\n",
        "                    synonyms if\n",
        "                    syn != word and syn not in all_stopwords and wordnet.wup_similarity(wordnet.synsets(word)[0],\n",
        "                                                                      wordnet.synsets(syn)[0]) is not None and\n",
        "                    wordnet.wup_similarity(wordnet.synsets(word)[0], wordnet.synsets(syn)[0]) > 0.5]\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "    # Remove duplicates\n",
        "    unique_similarities = []\n",
        "    seen = set()\n",
        "    for sim in similarities:\n",
        "        if sim[0] not in seen:\n",
        "            unique_similarities.append(sim)\n",
        "            seen.add(sim[0])\n",
        "    print(f'token : {word}, sim words: {unique_similarities}')\n",
        "    return unique_similarities[:n]\n",
        "\n",
        "query = \"Who is considered the \\\"Father of the United States\\\"?\"\n",
        "query_tokens = tokenize(query)\n",
        "print(query_tokens)\n",
        "sim_query_tokens =[]\n",
        "for token in query_tokens:\n",
        "    sim_tokens = [t[0] for t in find_most_similar(token)]\n",
        "    sim_query_tokens.extend(sim_tokens)\n",
        "sim_query_tokens.extend(query_tokens)\n",
        "print(sim_query_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPrqAP5SoRmJ",
        "outputId": "1f177378-9283-43d4-bb0e-238c823e190e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['considered', 'father', 'united', 'states']\n",
            "['considered', 'father', 'united', 'states']\n",
            "token : considered, sim words: [('consider', 1.0)]\n",
            "token : father, sim words: [('begetter', 1.0), ('mother', 0.9230769230769231), ('forefather', 0.782608695652174), ('beginner', 0.5454545454545454)]\n",
            "token : united, sim words: [('unite', 1.0)]\n",
            "token : states, sim words: [('state', 1.0), ('province', 1.0), ('commonwealth', 0.9411764705882353)]\n",
            "['consider', 'begetter', 'mother', 'forefather', 'beginner', 'unite', 'state', 'province', 'commonwealth', 'considered', 'father', 'united', 'states']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment the code below and execute to reload the module when you make\n",
        "# changes to search_frontend.py (after you upload again).\n",
        "import importlib\n",
        "importlib.reload(sim_calc)\n",
        "importlib.reload(se)\n",
        "importlib.reload(inverted_index_title_gcp)\n",
        "importlib.reload(inverted_index_body_gcp_training)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B63hc_d8vqQs",
        "outputId": "ec95d574-3e80-496f-e2c4-92e6e7ba9148"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'inverted_index_body_gcp_training' from '/content/inverted_index_body_gcp_training.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from time import time\n",
        "# url = 'http://35.232.59.3:8080'\n",
        "# place the domain you got from ngrok or GCP IP below.\n",
        "url = \"http://8a8b-34-106-45-248.ngrok-free.app\"\n",
        "qs_res = []\n",
        "counter = 0\n",
        "for q, true_wids in queries.items():\n",
        "  duration, ap = None, None\n",
        "  t_start = time()\n",
        "  try:\n",
        "      res = requests.get(url + '/search', {'query': q}, timeout=35)\n",
        "      duration = time() - t_start\n",
        "      if res.status_code == 200:\n",
        "        pred_wids, score = zip(*res.json())\n",
        "        print(f'{pred_wids[0]} , score: {score[0]}, time: {duration}')\n",
        "        rq = results_quality(true_wids, pred_wids)\n",
        "        break\n",
        "  except Exception as e:\n",
        "    # Handle the exception and print it\n",
        "    # print(\"An exception occurred:\", e)\n",
        "    pass\n",
        "\n",
        "  qs_res.append((q, duration))\n",
        "  print(f'q,time {qs_res} and {rq}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvUfLyHuHb0J",
        "outputId": "62563512-a8bd-44d2-fa0f-a6bcc5a82cc4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17457578 , score: 1.0, time: 0.6930637359619141\n",
            "0.0\n",
            "0.0\n"
          ]
        }
      ]
    }
  ]
}